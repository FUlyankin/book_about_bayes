

%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{extreport}

%%%%%%%%%% Програмный код %%%%%%%%%%
\usepackage{minted}
% Включает подсветку команд в программах!
% Нужно, чтобы на компе стоял питон, надо поставить пакет Pygments, в котором он сделан, через pip.

% Для Windows: Жмём win+r, вводим cmd, жмём enter. Открывается консоль.
% Прописываем easy_install Pygments
% Заходим в настройки texmaker и там прописываем в PdfLatex:
% pdflatex -shell-escape -synctex=1 -interaction=nonstopmode %.tex

% Для Linux: Открываем консоль. Убеждаемся, что у вас установлен pip командой pip --version
% Если он не установлен, ставим его: sudo apt-get install python-pip
% Ставим пакет sudo pip install Pygments

% Для Mac: Всё то же самое, что на Linux, но через brew.

% После всего этого вы должны почувствовать себя тру-программистами!
% Документация по пакету хорошая. Сам читал, погуглите!



%%%%%%%%%% Математика %%%%%%%%%%
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}
\mathtoolsset{showonlyrefs=true}  % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумерация формул слева



%%%%%%%%%%%%%%%%%%%%%%%% Шрифты %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[english, russian]{babel} % выбор языка для документа
\usepackage[utf8]{inputenc} % задание utf8 кодировки исходного tex файла
\usepackage[X2,TS1, T2A]{fontenc}        % кодировка
\usepackage{cmap}


\usepackage{fontspec}         % пакет для подгрузки шрифтов
\setmainfont{Linux Libertine O}   % задаёт основной шрифт документа

\usepackage{unicode-math}     % пакет для установки математического шрифта
\setmathfont[math-style=upright]{Neo Euler} % шрифт для математики


%%%%%%%%%% Работа с картинками %%%%%%%%%
\usepackage{graphicx}                  % Для вставки рисунков
\usepackage{graphics}
\graphicspath{{images/}{pictures/}}    % можно указать папки с картинками
\usepackage{wrapfig}                   % Обтекание рисунков и таблиц текстом


%%%%%%%%%% Работа с таблицами %%%%%%%%%%
\usepackage{tabularx}            % новые типы колонок
\usepackage{tabulary}            % и ещё новые типы колонок
\usepackage{array}               % Дополнительная работа с таблицами
\usepackage{longtable}           % Длинные таблицы
\usepackage{multirow}            % Слияние строк в таблице
\usepackage{float}               % возможность позиционировать объекты в нужном месте
\usepackage{booktabs}            % таблицы как в книгах!
\renewcommand{\arraystretch}{1.3} % больше расстояние между строками

% Заповеди из документации к booktabs:
% 1. Будь проще! Глазам должно быть комфортно
% 2. Не используйте вертикальные линни
% 3. Не используйте двойные линии. Как правило, достаточно трёх горизонтальных линий
% 4. Единицы измерения - в шапку таблицы
% 5. Не сокращайте .1 вместо 0.1
% 6. Повторяющееся значение повторяйте, а не говорите "то же"
% 7. Есть сомнения? Выравнивай по левому краю!

%  вычисляемые колонки по tabularx
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcolumntype{Y}{>{\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}

% межстрочный отступ в таблице
\renewcommand{\arraystretch}{1.2}


%%%%%%%%%% Графика и рисование %%%%%%%%%%
\usepackage{tikz, pgfplots}  % язык для рисования графики из latex'a


%%%%%%%%%% Гиперссылки %%%%%%%%%%
\usepackage{xcolor}              % разные цвета

% Два способа включить в пакете какие-то опции:
%\usepackage[опции]{пакет}
%\usepackage[unicode,colorlinks=true,hyperindex,breaklinks]{hyperref}

\usepackage{hyperref}
\hypersetup{
	unicode=true,           % позволяет использовать юникодные символы
	colorlinks=true,       	% true - цветные ссылки, false - ссылки в рамках
	urlcolor=blue,          % цвет ссылки на url
	linkcolor=black,          % внутренние ссылки
	citecolor=black,        % на библиографию
	pdfnewwindow=true,      % при щелчке в pdf на ссылку откроется новый pdf
	breaklinks              % если ссылка не умещается в одну строку, разбивать ли ее на две части?
}

%%%%%%%%%% Другие приятные пакеты %%%%%%%%%
\usepackage{multicol}       % несколько колонок
\usepackage{verbatim}       % для многострочных комментариев
\usepackage{cmap}           % для кодировки шрифтов в pdf

% свешиваем пунктуацию
% теперь знаки пунктуации могут вылезать за правую границу текста, при этом текст выглядит ровнее
\usepackage{microtype}

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos --- печатает все поставленные \todo'шки


%%%% Оформление %%%%%%%
% размер листа бумаги
\usepackage[
paperwidth=160mm,
paperheight=220mm,
headheight=14mm,
left=10mm,
right=10mm,
top=20mm,
bottom=20mm
]{geometry}

\usepackage{indentfirst}       % установка отступа в первом абзаце главы!!!

\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[LO]{\leftmark}
\fancyhead[RE]{\rightmark}


\usepackage{setspace}
%\setstretch{1.3}  % Межстрочный интервал
%\setlength{\parindent}{1.5em} % Красная строка.
%\setlength{\parskip}{4mm}   % Расстояние между абзацами
% Разные длины в латехе https://en.wikibooks.org/wiki/LaTeX/Lengths

% \flushbottom                            % Эта команда заставляет LaTeX чуть растягивать строки, чтобы получить идеально прямоугольную страницу
\righthyphenmin=2                       % Разрешение переноса двух и более символов
\widowpenalty=300                     % Небольшое наказание за вдовствующую строку (одна строка абзаца на этой странице, остальное --- на следующей)
\clubpenalty=3000                     % Приличное наказание за сиротствующую строку (омерзительно висящая одинокая строка в начале страницы)
\tolerance=10000     % Ещё какое-то наказание.

\usepackage{bm}
\usepackage{bbm} % шрифт с двойными буквами

% свешиваем пунктуацию
% теперь знаки пунктуации могут вылезать за правую границу текста, при этом текст выглядит ровнее
\usepackage{microtype}

% для эпиграфов
\usepackage{epigraph} 
\setlength\epigraphrule{0pt}
\renewcommand{\textflush}{flushepinormal}

% Внешний вид подписей к картинкам и таблицам
\usepackage[font=small, labelfont=bf]{caption}
\DeclareCaptionLabelSeparator{colon}{\textbf{.} }
\DeclareCaptionLabelFormat{dash}{#1\hspace{.55ex}#2}
\captionsetup[figure]{labelformat=dash}




%%%%%%%%%% Свои команды %%%%%%%%%%
\usepackage{etoolbox}    % логические операторы для своих макросов

% Математические символы первой необходимости:
\DeclareMathOperator{\sgn}{sign}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\E}{\mathop{E}}
\DeclareMathOperator{\Med}{Med}
\DeclareMathOperator{\Mod}{Mod}

\DeclareMathOperator*{\plim}{plim}

\newcommand{\const}{\mathrm{const}}        % const прямым начертанием

%% эконометрические сокращения
\def \hb{\hat{\beta}}
\def \hs{\hat{s}}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \he{\hat{\varepsilon}}
\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}

% Греческие буквы
\def \a{\alpha}
\def \b{\beta}
\def \t{\tau}
\def \dt{\delta}
\def \e{\varepsilon}
\def \ga{\gamma}
\def \kp{\varkappa}
\def \la{\lambda}
\def \sg{\sigma}
\def \tt{\theta}
\def \Dt{\Delta}
\def \La{\Lambda}
\def \Sg{\Sigma}
\def \Tt{\Theta}
\def \Om{\Omega}
\def \om{\omega}

% Готика
\def \mA{\mathcal{A}}
\def \mB{\mathcal{B}}
\def \mC{\mathcal{C}}
\def \mE{\mathcal{E}}
\def \mF{\mathcal{F}}
\def \mH{\mathcal{H}}
\def \mL{\mathcal{L}}
\def \mN{\mathcal{N}}
\def \mU{\mathcal{U}}
\def \mV{\mathcal{V}}
\def \mW{\mathcal{W}}

% Жирные штуки
\def \mbb{\mathbb}

\def \RR{\mbb R}
\def \NN{\mbb N}
\def \ZZ{\mbb Z}
\def \PP{\mbb{P}}
\def \QQ{\mbb Q}

% Карточные масти
\DeclareSymbolFont{extraup}{U}{zavm}{m}{n}
\DeclareMathSymbol{\varheart}{\mathalpha}{extraup}{86}
\DeclareMathSymbol{\vardiamond}{\mathalpha}{extraup}{87}


% Команды первой необходимости
\newcommand{\iid}{\mathrel{\stackrel{\rm i.\,i.\,d.}\sim}}  % ну вы поняли...
\newcommand{\fr}[2]{\ensuremath{^#1/_#2}}   % особая дробь
\newcommand{\ind}[1]{\mathbbm{1}_{\{#1\}}} % Индикатор события
\newcommand{\dx}[1]{\,\mathrm{d}#1} % для интеграла: маленький отступ и прямая d

\newcommand{\indef}[1]{\textbf{#1}}     % выделение ключевого слова в определениях

% бульпоинты в списках
\definecolor{myblue}{rgb}{0, 0.45, 0.70}
\newcommand*{\MyPoint}{\tikz \draw [baseline, fill=myblue,draw=blue] circle (2.5pt);}
\renewcommand{\labelitemi}{\MyPoint}

% для нормального распределения
\newcommand{\expp}[1]{ \exp \left( #1 \right)} 
% для прорисовки нормального распределения
\newcommand\gauss[2]{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))} 



%%%%%%%%%% Теоремы %%%%%%%%%%
\theoremstyle{plain}              % Это стиль по умолчанию.  Есть другие стили.
\newtheorem{theorem}{Теорема}[section]
\newtheorem{result}{Следствие}[theorem]
% счётчик подчиняется теоремному, нумерация идёт по главам согласованно между собой


\theoremstyle{definition}         % убирает курсив и что-то еще наверное делает ;)
\newtheorem*{definition}{Определение}  % нумерация не идёт вообще

\newtheorem{chudo}{Чудо номер}   % Для первой главы



%%%%%%%%%% Список литературы %%%%%%%%%%

%\usepackage[backend=biber,style=chem-acs,sorting=nty]{biblatex}
% style --- стиль оформления библиографии
% backend --- Движок для сборки. Просто пишите сюда biber. Trust me.
% sorting --- Порядок сортировки в списке. nty = сначала по имени, потом по названию, потом по году выхода статьи. В этот же список можно включить 'a' - по алфавиту,


%\addbibresource{bayes.bib} % сюда нужно вписать свой биб-файлик


%%%%%%%%%% Задачи и их решения %%%%%%%%%%%

\usepackage{answers}

\newtheorem{problem}{\color{myblue} Упражнение}
\Newassociation{sol}{solution}{solution_file}
% sol --- имя окружения внутри задач
% solution --- имя окружения внутри solution_file
% solution_file --- имя файла в который будет идти запись решений
% можно изменить далее по ходу

\setlength{\epigraphwidth}{0.5\textwidth}

\usepackage{pgf,tikz}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}

\begin{document}
	
	% \Opensolutionfile{solution_file}[solutions1]


\chapter{Кто-то теряет, а кто-то находит} 

\epigraph{Блин блинский! Это потеря потерь!}{Джейсон Стетхем}

В течение книги мы несколько раз говорили о том, что в байесовской статистике, на выходе, мы получаем гораздо больше, чем просто точечную оценку. Мы получаем целое распределение.  Также мы сказали, что если от нас требуется указать точечную оценку, мы должны спросить: <<А как нас накажут за ошибку?>> и выбрать точечную оценку исходя из этого. Вспомните одно из чудес первой главы. Для принцессы было важно дадут ей молодильное яблоко или превратят в тыкву. В зависимости от стимула она давала разный ответ. 

В этой главе мы поговорим о стимулах. Мы проанализируем классические функции потерь и увидим, что они предлагают нам в качестве прогнозов. Мы узнаем про энтропию, дивергенцию Кульбака-Лейблера, а также увидим, что правдоподобие обожает играть в шпионские игры и постоянно маскируется. Надеюсь, что нам удастся разоблачить его. 

\section{Про то какими бывают потери} 

Давайте представим себе машину. Она тормозит. Потому что пешеходный переход. Длина её тормозного пути зависит от разных факторов: скорости, гололёда, марки машины, шипастости шин и тп. Представим себе, что мы постоянно наблюдаем за одной и той же машиной на одной и той же дороге в одних и тех же условиях. В общем говоря, длина её тормозного пути $y$ зависит только от скорости $x$ с каким-то коэффициентом $\b$, то есть 

\[ y = \beta x + \e. \]

В данном случае $\e$ это шум, который накладывается на нашу взаимосвязь. В него входят различные случайные факторы, влияющие на тормозной путь (выскочившая белка, заевшая педаль и тп.).  Если мы хорошо грамотно специфицировали модель, то математическое ожидание шума равно нулю. 

У нас есть выборка. Мы немного понаблюдали за машиной и записали кучу измерений $(x_i, y_i)$. Осталось только оценить коэффициент $\beta$.  Возникает резонный вопрос: как это сделать?

Ответ прост. Решить насколько для нас страшно ошибиться в прогнозировании $y$ и ввести функцию потерь. Обычно выбор конкретного вида функции зависит от поставленной задачи.  Так в эконометрике обычно выбирается квадратичная функция потерь. Оценка коэффициента находится путём минимизации квадрата ошибки, допущенной при прогнозировании

\[ (y - \hat y)^2 = (y - \b x)^2 \to \min_{\b}. \]

Давайте попробуем в явном виде проминимизировать такую функцию. Для каждого из наших наблюдений ошибка прогноза должна быть как можно меньше. То есть нужно минимизировать суммарную ошибку прогноза 

\[  \sum_{i=1}^n(y_i - \b x_i)^2  \to \min_{\b}.  \]

Берём производную, решаем уравнение, получаем ответ

\begin{equation*}
2 \cdot \left(\sum y_i x_i - \b \sum x_i^2  = 0 \right) \Rightarrow \hb = \frac{\sum x_i y_i}{\sum x_i^2}.
\end{equation*}

Взяв вторую производную, можно убедиться, что это действительно минимум.  Сразу же после того, как была получена формула для оценивания бэтки\footnote{Обычно в англоязычной литературе такая формула называется estimator, то есть оцениватель. Конкретная оценка называется estimate. Почему-то богатый русский язык не впитал это различие и стал называть оценкой и формулу и конкретное численной значение. Давайте исправлять это недоразумение и называть формулы оценивателями.} возникает вполне естественный вопрос: откуда вообще взялась эта идея, минимизировать сумму квадратов отклонений?  Конечно, чем больше ошибка в прогнозе, тем сильнее нас карают за неё, но почему мы не взяли сумму модулей или четвёртых степеней?  Чтобы ответить на этот вопрос, нужно ввести несколько вероятностных предположений. 

Пусть ошибка в нашей регрессии  зашумляет истинную взаимосвязь между переменными по нормальному распределению $\e \sim N(0, \sigma^2)$. Тогда мы можем выписать для нашей задачки функцию максимального правдоподобия

\[ L =  \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} e^{- \frac{\e_i^2}{2 \sigma^2}   } =  \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} e^{- \frac{(y_i - \b x_i)^2}{2 \sigma^2}   } \]

Прологорифмируем полученное добро

\[ \ln L = -\frac{n}{2} \cdot \ln(2 \pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \b x_i)^2. \]

Что мы видим? Для максимизации логарифма правдоподобия необходимо минимизировать сумму квадратов отклонений. Выходит, что метод наименьших квадратов на деле оказывается замаскированным методом максимального правдоподобия, его частным случаем. На практике довольно часто функции потерь вытекают из каких-то функций правдоподобия. Убедиться в этом вы можете, попытавшись решить упражнение \ref{log_reg}. 

Вторым важным наблюдением оказывается то, что выбор функции ошибки и распределения шума как-то взаимосвязаны между собой.  Обратите внимание, что ошибки здесь имеют нулевое среднее и одинаковую дисперсию. Если вдруг мы увидим, что ошибки у нас имеют другую природу (ненулевое среднее или различные дисперсии), то с этим нужно что-то делать. Например, поискать другую функцию ошибки либо ввязаться в яростную борьбу с природой за предпосылки. 

Эконометрика обычно проповедует путь борьбы. Дело в том, что оценки наименьших квадратов, при соблюдении предпосылок, обладают рядом няшных статистических свойств. Эти свойства открывают для нас целый мир, связанный с проверкой гипотез о различных взаимосвязях между переменными. 

При этом главным профитом статитических процедур, проведуемых в эконометрике,  является величина эффекта. На выходе мы получаем величину $\hb$, которую можно проинтерпретировать. Например, в нашем случае она будет означать, что при увеличении скорости на единицу, при прочих равных в среднем длина тормозного пути увеличивается на $\hb$. 

Как только мы немного видоизменим функуцию потерь, например добавим для борьбы с переобучение регуляризатор, интерпретация сразу же будет утеряна. Дело в том, что регуляризация для улучшения прогнозных свойств модели вносит в неё искусственное смещение.  Не будем забегать вперёд. Разговор о регуляризации и о двух великих вопросах анализа данных ожидает нас в следущей главе. Постарайтесь удержать  мысль о взаимосвязи между вероятностными распределениями и функциями потерь у себя в голове до следущей главы. Там она получит должное развитие. Здесь же мы собрались немного по иному поводу. 

Интерпретация --- это хорошо. Однако не стоит сковывать себя жесткими обязательствами.  Мы свободны сами выбирать свою судьбу\footnote{Если читатель внимательно читал вторую главу, он помнит, что это неточно.}. Никто не вынуждает нас останавливаться именно на такой функции потерь.  Мы можем взять и использовать для решения задачи сумму модулей отклонений

\[ |y - \hat y| = |y - \b x| \to \min_{\b}. \]

Конечно же мы потеряем няшные статистические свойства. Но тем не менее никто не мешает нам обратиться к великому и могучему бустрапу и сбутсрапировать все доверительные интервалы и все критические значения для статистик, если это нам неожиданно потребуется. 

Чаще всего условия выбора нам диктует задача, вставшая перед нами. Функцию потерь иногда приходится как следует выстрадать. Например, если речь идёт о числе товаров, которые мы должны хранить на складе, возникает необходимость использовать несимметричную кусочную функцию потерь.  

Если мы завезли на склад слишком мало товара, потребителям не хватит его. Из-за того, что на товар будет наценка, а также из-за его нехватки, мы потеряем лояльность клиентов. Кривая потерь пойдёт под одним углом. Если нехватка будет небольшой, мы покроем её из запасов, потерь не будет. Если на складе будет избыток товара, мы потратим деньги на его хранение, кривая пойдёт под другим углом. Если избыток будет очень сильным, то возникнет просрочка. Кривая пойдёт под третьим углом.  

\begin{center}
\begin{tikzpicture}
% оси
\draw [->] (-4,0) -- (4,0);
\draw [->] (0,-0.2) -- (0,4);	
\node [below right] at (3, 0.6) {$\text{ошибка прогноза (объём)}$};
\node at (0.75,4) {$\text{потери}$};

%% кривая
\draw [blue, thick, domain = -4:-2] plot (\x, {-0.25*\x - 0.5});
\node [below] at (-3.2,-0.1) {\footnotesize $\text{лояльность}$};
\draw [blue, thick, domain = -2:0] plot (\x, 0);
\node [below] at (-1.2,-0.1) {\footnotesize $\text{запасы}$};
\draw [blue, thick, domain = 0:2] plot (\x, {0.5*\x});
\node [below] at (1,-0.1) {\footnotesize $\text{хранение}$};
\draw [blue, thick, domain = 2:4] plot (\x, {1.5*\x - 2});
\node [below] at (3.2,-0.1) {\footnotesize $\text{просрочка}$};
\end{tikzpicture}	
\end{center}


Одним словом говоря, функции потерь бывают разные. А свобода выбора --- это очень страшно\footnote{Вот так свобода и умирает под гром аплодисментов.}. Хочется с ним не ошибиться и осознавать, какая функция потерь к каким последствиям (в плане прогнозов) может привести.  Давайте попробуем это понять, а после будем реагировать на стимулы как Спящая Красавица. 

Для разнообразия пока что не будем говорить о $y$ и $\hat y$ (вернёмся к ним через страницу). В байесовском мире у нас есть $\beta$ с каким-то апостериорным распределением. И мы хотели бы выбрать из него точечную оценку $\hat \beta$. 

Пусть $L(\beta, \hat \beta)$ --- наша функция потерь, наказание, которое мы несём за ошибку. Тогда наш выбор оценки заключается в минимизации апостериорных ожидаемых потерь, то есть 

\[ \E( L(\beta, t) \mid y) = \int L(\beta,t) \cdot f(\beta \mid y) \dx{\b} \to \min_{t}. \]

Будем перебирать все возможные оценки $t$ так, чтобы минимизировать математическое ожидание функции потерь. Обратим внимание на то, что случайной величиной в данном случае является $\beta$. Значение $t$ мы пытаемся выбрать самостоятельно.  Давайте попробуем подставить в этот интеграл конкретные функции потерь и посмотрим что у нас из этого получится. 

\section{Про квадратичные потери потерь}

Пусть $L(\beta, \hat \beta) = (\hat \beta - \beta)^2$, тогда 

\[ \int (\beta - t)^2 \cdot f(\beta \mid y) \dx{\beta} \to \min_{t} \] 

Найдём производную по $t$

\begin{align*}
\frac{\partial }{\partial t} \left(  \int (\beta - t)^2 \cdot f(\beta \mid y) \dx{\beta}   \right) &= - 2 \cdot \int (\beta - t) \cdot f(\beta \mid y) \dx{\beta} = 0 \\ 
\int \beta \cdot f(\beta \mid y) \dx{\beta} - \int  t \cdot f(\beta \mid y) \dx{\beta} &= \E(\beta \mid y) - t \cdot 1 =  0 \Rightarrow  t = \E(\beta \mid y) 
\end{align*}

Из этих довольно простых рассуждений мы получаем несколько интересных выводов. Во-первых, при квадратичных потерях, оптимальной байесовской точечной оценкой будет апостериорное среднее. 

<<Во-вторых>> связано с машинным обучением и эконометрикой. Предположим, нам захотелось методом наименьших квадратов обучить модель и получить исходя из неё прогноз $\hat y$. За каждый плохой прогноз мы понесём наказание $(y_i - \hat y_i)^2$. Выходит, что в такой ситуации наилучшим прогнозом будет условное математическое ожидание. 

Ошибка на обучающей выборке $\frac{1}{n} \cdot \sum_{i=1}^n L(y_i, \hat y_i)$ --- это просто эмпирическая оценка ожидаемых потерь $\E(L(y, \hat y \mid x)$. Этот факт позволяет по-новому взглянуть на старые функции потерь. Минимизируя $\E(L(y, \hat y \mid x)$, можно понять что именно мы получаем на выходе в качестве оценки. В случае квадратичной функции потерь, это ни что иное как $\E(y \mid x)$. Убедиться в этом можно, проделав ровно то же самое, что мы проделали выше, но заменить $\b$ на $y$, а $\hb$ на $\hat y$

\[ \E((y-t)^2 \mid x) = \int (y - t)^2 \cdot f(y \mid x) \dx{y} \to \min_{t}.\] 

Подобный анализ позволяет иногда обнаружить, что функция потерь для решения задачи была выбрана не очень удачно. Например, в упражнении \ref{abs_0_1} целевая переменная $y$ принимает значения $0$ и $1$. Нам хочется оценить вероятность того, что $y$ примет значение $1$. Хотелось бы, чтобы модель выдавала нам число, лежащее на отрезке от нуля до единицы. Решив упражнение, можно убедиться, что использовать для этих целей абсолютное отклонение, не очень удачная идея, так как оно будет выдавать в качестве ответа либо $0$ либо $1$, но никак не вероятность.   


\section{Про абсолютные потери потерь} 

Пусть $L(\beta,  \hat \beta) = |\hat \beta - \beta|$, тогда

\[ \int |\beta - t| \cdot f(\beta \mid y) \dx{\beta} \to \min_{t}. \] 

Найдём производную по $t$, при этом не будем забывать, что в нуле модуль не дифференцируется. К счастью, так как $\PP(\beta = t \mid y) = 0$, одна точка никак не повлияет на наш интеграл.

\begin{multline*}
\frac{\partial }{\partial t} \left(  \int |\beta - t| \cdot f(\beta \mid y) \dx{\beta}   \right) = \frac{\partial }{\partial t} \left(  \int_{\beta \neq t} |\beta - t| \cdot f(\beta \mid y) \dx{\beta}   \right) = \\ = \frac{\partial }{\partial t} \left( \int_{\beta > t} (\beta - t) \cdot f(\beta \mid y) \dx{\beta} - \int_{\beta < t} (\beta - t) \cdot f( \beta \mid y) \dx{\beta}  \right) = \\
= \int_{\beta < t} f(\beta \mid y) \dx{\beta} - \int_{\beta > t} f( \beta \mid y) \dx{\beta} = 0
\end{multline*}

Получается, что в данном случае для минимизации ожидаемых потерь, нужно, чтобы $\PP(\beta < t \mid y) = \PP(\beta > t \mid y)$. Ни для кого не секрет, что точка, в которой выполнится такое равенство, называется апостериорной медианой. Получаем, что $t = \Med(\beta \mid y)$. 

Снова делаем два вывода. Во-первых, в случае такой функции потерь апостериорная медиана является оптимальной байесовской точечной оценкой. Во-вторых, в случае, если мы используем такую функцию потерь для обучения модели по признаку $x$ и ответу $y$, то алгоритм в качестве оценки будет выдавать нам $\Med(y \mid x)$.  

Надеемся, что для читателя теперь стало понятно, почему абсолютная ошибка нечувствительна к выбросам. На медиане они практически никак не сказываются, и прогноз не портится. Квадратичная ошибка к выбросам очень чувствительна. Одно большое значение довольно сильно искажает среднее.

\section{Про жадные потери потерь} 

%Следущая функция потерь называется жадной. Если мы угадываем, то мы получаем всё. Если мы не угадываем, то мы не получаем ничего. 
%
%\[ L(\beta, \hat \beta) = \begin{cases} 1, \text{ежели мы не прогадали и }  \hat \beta = \beta \\ -1, \text{коли накладочка вышла и } \hat \beta \neq \beta    \end{cases} \]

%Чтобы не запутаться, давайте будем считать, что $\b$ принимает конечное число значений. Мы, по-прежнему, хотим минимизировать своей оценкой ожидаемые потери

%\[ \PP(t = \hat \b \mid y) - \sum_{i \ne \hat \b} \PP(t = i \mid y) \to \min_{t} \]

%\[ \int_{\beta = t} 1 \cdot f(\beta \mid y) \dx{\beta} + \int_{\beta \neq t} -1 \cdot f(\beta \mid y) \dx{\beta} \to \min_{t} \] 

%В предыдущем пункте вероятность попадания случайной величины в точку была нулевой, $\PP(\beta = \beta_F \mid y) = 0$. С того момента мало что изменилось. Первый интеграл зануляется. Дополнительно вспомним о том, что искать минимум функции, умноженной на минус единицу, это то же самое, что искать максимум обычной функции. Получается, что наша задача превращается в задачу 

%\[ \int_{\beta \neq t} f(\beta \mid y) \dx{\beta} \to \max_{t}.\] 

\todo[inline]{Я запутался}


\section{Про логистические потери потерь}

Пусть целевая переменная $y$ принимает значения $0$ и $1$.  Нам хочется по переменной $x$ научиться прогнозировать $y$. Такая задача называется \indef{классификацией.}  На самом деле мы уже решали такую задачу в упражнениях для прошлой главы, когда говорили о поломке шатла. 

Задачу классификации можно попробовать решить методом максимального правдоподобия. Целевая переменная $y$ принимает значение $1$ с вероятностью $p$ и значение $0$ с вероятностью $1-p$.  Если у нас завалялась выборка $y_1, \ldots, y_n$, то по всем законам жанра можно выписать функцию правдоподобия: 

\[ L = \prod_{i=1}^n {p^{y_i} \cdot (1-p)^{1-y_i} }= p^{\sum y_i} \cdot (1 - p)^{\sum (1 - y_i)}\]

Прологарифмируем функцию правдоподобия и получим

\[ \ln L = \sum y_i \cdot \ln p + \sum(1 - y_i) \cdot \ln (1-p) = \sum_{i=1}^n y_i \ln p + (1- y_i) \ln (1-p).\]
 
Выходит, что чтобы максимизировать функцию правдоподобия, нам нужно минимизировать следущую функцию потерь

\[L(y, \hat y) = -y \cdot \ln (\hat  y) - (1-y) \cdot ln(1 - \hat y). \] 

Эта функция потерь называет \indef{логистической (logloss).} Она часто используется в машинном обучении и эконометрике при решении задачи классификации.  Вероятность $p$ в данной модели как-то должна зависеть от регрессора $x$.  Функция, описывающая эту зависимость должна принимать значения на отрезке $[0;1]$. В качестве такой функции берут какую-нибудь функцию распределения.  Обычно это логистическое распределение (иногда функцию распределения логистической случайной величины называют \indef{сигмоидой})

\[ P(y = 1 \mid x) = p =  \frac{1}{1 + e^{-\b x}}.\]

При использовании сигмоиды оценки коэффициентов имеют интересную интерпретацию. Если мы найдём логарифм отношения шансов, то мы получим, что 

\[ \ln \frac{p}{1-p} = \b x. \]

Выходит, что при изменении $x$ на единицу, логарифм отношения шансов изменяется на $\b$, то есть шансы на то, что $y=1$ изменяются на $100 \cdot \b \%$. При других функциях потерь хорошую интерпретацию для коэффициентов получить довольно сложно.

Мы немного отвлеклись от генеральной линии повествования. Давайте вернёмся к ней. Когда мы конструировали логистические потери потерь, исходя из принципа правдоподобия, мы сразу же заложили в их природу то, что на выход в качестве прогноза будет идти вероятность $\PP(y = 1)$. Тем не менее, давайте сделаем вид, что мы забыли это и проанализируем функцию потерь также, как мы делали это выше. 

Мы хотели бы минимизировать условное математическое ожидание  $\E(L(y, \hat y) \mid x)$. Случайной величиной в данном случае является переменная $y$, которая принимает два значения. Выписываем математическое ожидание

\[ 
\sum_{k \in Y} (-y \ln t - (1-y) \ln(1-t)) \PP(y = k \mid x) \to \min_t.
\] 

Обозначим для удобства $\PP(y = 1 \mid x)$ как $p$. Тогда, учитывая что $Y = \{0,1\}$, наша задача примет вид 

\[ 
-(1-p) \cdot \ln(1-t) - p \cdot \ln(t) \to \min_t.
\]

Дело осталось за малым. Берём производную и находим экстремум. 

\begin{multline*}
\frac{\partial }{\partial t} \left(  -(1-p) \cdot \ln(1-t) - p \cdot \ln(t) \right) = \\ = \frac{1-p}{1-t} - \frac{p}{t} = 0   \Rightarrow t = p = \PP(y = 1 \mid x).
\end{multline*}

Выходит, что в логистической регрессии, минимизируя рассмотренную выше функцию потерь, мы получаем именно оценку вероятности.  Думаю, что концепция, в принципе, ясна. Не будем топтаться на месте и перейдём к следующему сюжету. 

\section{ Про энтропию} 

Приятно было бы начать этот раздел с картинки. Давайте посмотрим на распределение двух случайных величин. Какая из них предсказуемее: левая или правая? 

\begin{figure}[H]
\begin{minipage}[H]{0.49\linewidth}
	\begin{tikzpicture}
	% оси
	\draw [->] (-1.8,0) -- (4,0);
	\draw [->] (0,-0.2) -- (0,3.5);
	% график
	\draw [blue, thick, domain=0:3] plot (\x, 2);
	\draw [->, blue, thick] (-1.8,0)--(-0.05,0);
	\draw [<-, blue, thick] (3.05,0)--(4,0);
	\draw [blue, thick,dashed] (3,0)--(3,2);
	% точки
	\draw[fill,blue] (3,2) circle [radius=0.03];f
	\draw[fill,blue] (0,2) circle [radius=0.03];
	% подписи
	\node [below] at (0.2,0) {0};
	\node [below] at (3,0) {3};
	\node [left] at (0,2) {$\frac{1}{3}$};
	\node [below right] at (4,0) {$y$};
	\node [left] at (0,3.3) {$f_Y(y)$};
	\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}[H]{0.49\linewidth}
	\begin{tikzpicture}
	% оси
	\draw [->] (-2.8,0) -- (2,0);
	\draw [->] (-1,-0.2) -- (-1,3.5);
	% график12
	\draw [blue, thick, domain=0:1] plot (\x, {12*\x*\x*(1-\x)});
	\draw [blue, thick] (-2.8,0)--(0,0);
	\draw [blue, thick] (1,0)--(2,0);
	%\draw [blue, thick,dashed] (2,0)--(2,2);
	% точки
	%\draw[fill,blue] (2,2) circle [radius=0.03];
	%\draw[fill,blue] (0,2) circle [radius=0.03];
	% подписи
	\node [below] at (0,0) {1};
	\node [below] at (1,0) {2};
	% \node [left] at (0,1) {1};
	\node [below right] at (2,0) {$z$};
	\node [left] at (-1,3.3) {$f_Z(z)$};
	\end{tikzpicture}
\end{minipage}
\end{figure} 

Случайная величина $Z$ (правая) сконцентрирована на довольно узком отрезке. Вероятность того, что она выпадет за его пределами крайне мала. Случайная величина $Y$ (левая) сконцентрирована на широком отрезке. Она равновероятно может выскочить из любой его части. Логика подсказывает, что она непредсказуемее. Если мы решим спрогнозировать эти две случайные величины, правый случай нам будет обуздать легче. Ошибка, которую мы будем допускать, окажется меньше чисто из-за пикообразной природы этой случайной величины. 

Давайте теперь посмотрим на ещё две картинки. Какая непредсказуемее: левая или правая? 

\begin{figure}[H]
	\begin{minipage}[H]{0.49\linewidth}
		\begin{tikzpicture}
		% оси
		\draw [->] (-1.8,0) -- (4,0);
		\draw [->] (0,-0.2) -- (0,3.5);
		% график
		\draw [blue, thick, domain=0:2] plot (\x, 2);
		\draw [->, blue, thick] (-1.8,0)--(-0.05,0);
		\draw [<-, blue, thick] (2.05,0)--(4,0);
		\draw [blue, thick,dashed] (2,0)--(2,2);
		% точки
		\draw[fill,blue] (2,2) circle [radius=0.03];
		\draw[fill,blue] (0,2) circle [radius=0.03];
		% подписи
		\node [below] at (0.2,0) {0};
		\node [below] at (2,0) {1};
		\node [left] at (0,2) {1};
		\node [below right] at (4,0) {$y$};
		\node [left] at (0,3.3) {$f_Y(y)$};
		\end{tikzpicture}
	\end{minipage}
	\hfill
	\begin{minipage}[H]{0.49\linewidth}
		\begin{tikzpicture}
% оси
\draw [->] (-1.8,0) -- (4,0);
\draw [->] (0,-0.2) -- (0,3.5);
% график
\draw [blue, thick, domain=1:3] plot (\x, 2);
\draw [->, blue, thick] (-1.8,0)--(0.96,0);
\draw [<-, blue, thick] (3.05,0)--(4,0);
\draw [blue, thick,dashed] (3,0)--(3,2);
\draw [blue, thick,dashed] (1,0)--(1,2);
% точки
\draw[fill,blue] (3,2) circle [radius=0.03];f
\draw[fill,blue] (1,2) circle [radius=0.03];
% подписи
\node [below] at (1.2,0) {1};
\node [below] at (3,0) {2};
\node [left] at (0,2) {1};
\node [below right] at (4,0) {$z$};
\node [left] at (0,3.3) {$f_Z(z)$};
\end{tikzpicture}
	\end{minipage}
\end{figure} 

Случайные величины $Y$ и $Z$ отличаются друг от друга только отрезком. Одна распределена от $0$ до $1$, вторая от $1$ до $2$. Их форма одинакова. Они принимают разные значения, но одинаково непредсказуемо. Если мы попробуем спрогнозировать их, сложность этой затеи будет одинакова. 

Для того, чтобы улавливать такие вещи придумали специальную метрику. Она называется энтропией.  \indef{Энтропия} --- это мера непредсказуемости случайной величины $Y$, это то количество информации, которое я получаю, наблюдая случайную величину $Y$.  Она никак не опирается на те значения, которые принимает случайная величина и для дискретного случая определяется  как 

\[ H(Y) = \E(- \ln \PP(Y)). \]

Для непрерывного случая  энтропия определяется как 

\[ H(Y) = \E(- \ln f_Y(y)). \]

Попробуем немного пожить с энтропией. После того, как мы освоимся, можно будет поглубже обсудить её смысловую составляющую. Посчитаем энтропию для случайной величины:

\begin{center}
	\begin{tabular}{c|c|c|c}
		$Y$ & $1$ & $17$  &  $26$  \\ \hline
		$\PP(Y = k)$ & $\frac{1}{3}$ & $\frac{1}{3}$  & $\frac{1}{3}$ 
	\end{tabular}
\end{center}

Энтропия никак не смотрит на то, какие именно значения принимает случайная величина. Её интересует только то, как вероятность размазана по этим значениям:

\[ H(Y) = - \frac{1}{3} \cdot \ln \left(\frac{1}{3}\right)- \frac{1}{3} \cdot \ln \left(\frac{1}{3}\right) - \frac{1}{3} \cdot \ln \left(\frac{1}{3}\right) = \ln 3.\] 

Для случайной величины, принимающей $4$ значения с вероятностями $\frac{1}{4}$ энтропия будет равна $\ln 4$, а в общем случае для $Y \sim U[0;a]$ энтропия составит 

\[H(Y) = \int_0^a \frac{1}{a} \cdot (- \ln\left(\frac{1}{a}\right)) \dx{t} = \ln a. \]

Чем больше значений принимает равномерная случайная величина, тем она непредсказуемее.  Кстати говоря, для вырожденного распределения 

\begin{center}
	\begin{tabular}{c|c}
		$Y$ & $42$   \\ \hline
		$\PP(Y = k)$ & $1$
	\end{tabular}
\end{center}

энтропия окажется нулевой. Вырожденная случайная величина очень даже определена. Попробуем более сложную ситуацию, найдём энтропию для нормально распределённой случайной величины $Y \sim N(0, \sigma^2)$:

\begin{multline*}
H(Y) = \E( - \ln(f_Y(y)))  = \int_{-\infty}^{+\infty} f_Y(y) \cdot \ln f_Y(y) \dx{y} = \\ = \E\left(\frac{1}{2} \ln(2 \pi \sigma^2 ) + \frac{Y^2}{2\cdot \sigma^2}\right) = \frac{1}{2} \ln(2 \pi \sigma^2) + \frac{1}{2}.
\end{multline*} 

Если попробовать подставить в формулу разные значения $\sigma$, то можно получить следущую примерную табличку: 

\begin{center}
	\begin{tabular}{c|c|c|c}
		$\sigma$ & $1$ & $10$  &  $100$  \\ \hline
		$H(Y)$ & $\ln 4.13 $ & $\ln 41.3 $  & $\ln 413 $
	\end{tabular}
\end{center}

Выходит, что случайные величины $X \sim U[0;4]$ и $Y \sim N(0,1)$ в плане непредсказуемости очень похожи. Небольшой флэшбэк. Когда мы впервые говорили о априорных распределениях, мы сказали, что часто статистики говорят, что вообще ничего не знают о параметрах и выражают своё незнание либо нормальным распределением с большой дисперсией либо равномерным на очень большом отрезке. Энтропия в какой-то степени является обоснованием того, почему эти два подхода замоделировать своё априорное незнание эквивалентны. 

Энтропия обладает несколькими интересными математическими свойствами:

\begin{enumerate}
	\item  Она всегда неотрицательна $H(Y) \ge 0$.
	
	\item  Для конечного числа исходов $m$, равномерное распределение над этими исходами будет давать максимальную энтропию. 
		
	\item Если $X$ и $Y$ независимы, то $H(X \cdot Y) = H(X) + H(Y)$. 
	
	\item Если $X$ и $Y$ имеют одинаковые распределения, но принимают разные значения, то $H(X) = H(Y)$. Энтропия это функция над распределениями, а не значениями. 
\end{enumerate}

На самом деле формула для энтропии была получена как раз исходя из этих свойств\footnote{Помните мы недавно обсуждали как формула для простейшего потока события тоже была получена из свойств этого потока? Можно показать, что эта формула единственна. Тут такая же ситуация. Честно говоря, это восхищает.}.  Освоились? Давайте теперь поговорим про смысл. 

Итак, энтропия отражает то, насколько случайная величина непредсказуема. Другой характеристикой, описывающей вариабильность случайной величины является дисперсия. Отличие энтропии от дисперсии в том, что ей плевать на значения, которые принимает случайная величина. Если найти энтропию, для равномерного, нормального и экспоненциального распределений, которые мы использовали для моделирования наших априорных ожиданий чаще всего, то мы получим, что:

\begin{equation*}
\begin{aligned}
 N(0,\sigma^2):  \qquad  &H(Y)  = \frac{1}{2} \cdot \ln(2 \pi e) + \ln \sigma \\
 U[a;b]:         \qquad     &H(Y) = \frac{1}{2} \cdot \ln(12) + \ln \sigma,  \quad  \sigma^2 = \frac{1}{12}(b-a)^2 \\ 
 Exp(\alpha):      \qquad &H(Y) = 1 + \ln \sigma, \quad \sigma^2 = \alpha^{-2}\\
\end{aligned}
\end{equation*}

 Энтропия для всех трёх распределений выражается через дисперсию и может быть записана, как логарифм стандартного отклонения плюс некоторая константа. Этот факт заставляет начать сомневаться в целесообразности введения нового понятия.

На самом деле с целесообразностью всё в порядке. Для мультимодальных распределений обнаруженная нами закономерность нарушается. Можно довольно легко подобрать случайные величины $X$ и $Y$, для которых $\Var(X) > \Var(Y)$, но при этом $H(X) < H(Y)$. Всё бы хорошо, но это порождает новый вопрос. Какая из случайных величин, $X$ или $Y$ непредсказуемее? Какую метрику для этого использовать? В прочем, ответ вас не удивит. Выбор метрики зависит от задачи. 

\textbf{Ещё раз, ещё раз.}  Вся мощь энтропии заключена в том, что она описывает неопределённость, заложенную в случайную величину абстрагируясь от её значений и порядка этих значений.  Дисперсия, в свою очередь, очень пристально смотрит на значения случайной величины и их порядок. 

Давайте представим себе парочку ситуаций. 

\todo[inline]{Пример когда плевать на значения}

Пусть теперь на наш остров надвигается шторм. Хочется понять будет ли шторм разрушительным. Пусть $X$ это скорость ветра в километрах в час.   Давайте сравним между собой два распределения: 

\begin{enumerate}
\item $X_1 \sim N(100, 10^2)$. 

\item $X_2 \sim $  смесь $N(50, 3^2)$ и $N(200, 3^2)$ с весами $0.5$. 
\end{enumerate}

Как раз в данном случае одно из распределение бимодальное. Мы можем увидеть, что $\Var(X_1) <  \Var(X_2)$, но при этом $H(X_1) >  H(X_2)$, так как случайная величина $X_2$ более плотно сосредоточена на двух конкретных значениях.  С практической точки зрения было бы правильным говорить, что случайная величина $X_2$ несёт в себе большую неопределённость, так как  мы не знаем будет ли обычный ветер или разрушительный шторм. В случае $X_1$ мы знаем, что ветер будет сильным, но неразрушительным. В данном случае имеет смысл руководствоваться дисперсией, так как для нас важны значения, которые принимает случайная величина. 



\todo[inline]{Задачка про робота}

\textbf{Ещё раз, ещё раз.} При использовании в формуле энтропии двоичного логарифма, мы можем интерпретировать её как среднее количество бит информации, которое мы  тратим на кодирование. На практике нам обычно хочется полегче считать энтропию. Поэтому мы отойдём от двоичного основания назад к натуральному, так как с натуральными логарифмами работать намного приятнее. 

Энтропия довольно часто используется в машинном обучении. Например, с помощью неё обучают деревья.  Кроме того, на ней базируется понятие спутанности (perplexity), которое определяется как 

\[ Perplexity(Y) = e^{H(X)}\]

Перплексия (спутанность) довольно часто используется в моделях, связанных с обучением без учителя, но с этим мы встретимся позже. Сейчас наша дорога ведёт нас к очередному новому понятию, \indef{дивергенции Кульбака-Лейблера.}


\section{Про дивергенцию} 

Дивергенция Кульбака-Лейблера пришла в теорию вероятностей из теории информации. По сути KL-дивергенция --- это мера разницы между двумя вероятностными распределениями $P$ и $Q$.  Обычно считают, что $P$ --- это истиное распределение, а $Q$ --- его приближение.  Пусть распределение $P$ сложное и страшное и мы хотим заменить его на простое и хорошее. 

\todo[inline]{картинка с простым распределением и со страшным}


В таком случае дивергенция служит оценкой качества приближения и отражает то, какое количество информации мы потеряли, заменив распределение $P$ на распределение $Q$.  Обычно KL-дивергенцию обозначают как $KL(P || Q)$. 

Для дискретного случая дивергенцию можно найти как 

\[ KL(P || Q) = \sum  \PP(x) \cdot \ln \frac{ \PP(x)}{Q(x)}. \]

Для непрерывного случая формула аналогична, но сумма заменяется на интеграл, а вероятности на плотности


\[ KL(P || Q) = \int f_P(x) \cdot \ln \frac{ f_P(x)}{f_Q(x)}. \]

Выше мы сказали, что KL-дивергенция позволяет измерять расстояния между распределениями. Однако уже по формуле видно, что дивергенция Кульбака-Лейблера расстоянием не является.  Из курса матана долгопомнящий читатель может вспомнить, что для того, чтобы функция $\rho(x,y)$ была расстоянием, необходимо выполнение трёх свойств: 

\begin{enumerate}
\item   Неотрицательность: $\rho(x,y) \ge 0$. Расстояние от одного объекта до другого всегда положительно. Если расстояние равно нулю, то объекты находятся в одном месте. 

\item  Симметричность: $\rho(x,y) = \rho(y,x)$. От первого объекта до второго ехать столько же сколько обратно. 

\item  Неравенство треугольника: $\rho(x,y) + \rho(y,z) \ge \rho(x,z)$. Если мы ехали из одной точки в другую и по пути заехали куда-то ещё, то ехать придётся подольше, если наша остановка была нам не по пути. 
\end{enumerate}

С первым свойством всё хорошо. Со вторым и третьим начинаются проблемы. Дивергенция несимметрична, $KL(P || Q) \ne KL(Q || P)$. Зачем так сложно? Почему бы просто не взять и не использовать обычные давно известные метрики. Мало того, что они нам знакомы, так ещё и симметричны. Например, почему бы не взять $\rho(P,Q) = \max_t |f_P(x) - f_Q(x) |$? 

Для ответа на этот вопрос снова посмотрим на парочку картинок.

\todo[inline]{картинки}


На левой картинке расположены разные распределения. Однако  в терминах $\rho(P,Q)$ эти две плотности будут похожи.  Метрика забьёт на вероятностные различия и сконцентрируется на функциональных особенностях плотностей. На второй картинке наоборот для двух похожих распределений $\rho(P,Q)$ выдаст сильную разницу. Одним словом говоря, дивергенция хорошо зарекомендовала себя на практике даже несмотря на то, что она не является расстоянием в привычном для на смысле. Скорее даже наоборот, ассиметрия помогает нам, так как обычно мы всегда хотим заменить забубенистое распределение, которое мы видим в данных чем-то более простым, а это односторонняя ситуация. 

Поглядим на формулу для поиска энтропии чуть пристальнее и попробуем над ней немного поколдовать

\begin{multline*}
KL(P || Q) = \sum \PP(x) \cdot \ln \frac{ \PP(x)}{Q(x)} = \\ = \sum \PP(x) \ln \PP(x) - \sum \PP(x) \ln Q(x) = H(P) + H(P,Q).
\end{multline*}

Получается, что $KL$-дивергенция в явном виде выражается через энтропию распределения $P$.  Второе слагаемое называется \indef{перекрёстной или кросс энтропией}.  Перекрёстная энтропия интерпретируется как риск использования распределения $Q$, если данные пришли из $P$. 

Выходит, что оценка качества приближения распределения $P$ распределением $Q$ складывается из двух составляющих: энтропии $P$ и риска использования $Q$ вместо $P$. Смысл мы обсудили, теперь давайте попробуем пощупать $KL$-дивиргенцию ручками. 

Пусть у нас есть случайная величина $X$, имеющая распределение $P$. Это распределение кажется нам слишком сложным и мы хотим заменить его на $Q$. 

\begin{center}
	\begin{tabular}{c|c|c|c}
		$X$ & $1$ & $10$  &  $100$  \\ \hline
		$P$ & $\frac{1}{2}$  & $\frac{1}{4}$   &  $\frac{1}{4}$  \\ \hline 
		$Q$  &  $\frac{1}{3}$  &  $\frac{1}{3}$  &  $\frac{1}{3}$ 
	\end{tabular}
\end{center}


Давайте посчитаем дивергенцию между этими двумя распределениями. 

\begin{multline*}
KL(P || Q) = \left[ \frac{1}{2} \ln(3) + \frac{1}{4} \ln(3) + \frac{1}{4} \ln(3) \right] - \\ -\left[ \frac{1}{2} \ln(2) + \frac{1}{4} \ln(4) + \frac{1}{4} \ln(4) \right] = 0.059.
\end{multline*}

Теперь сделаем то же самое для непрерывной случайной величины. Найдём дивергенцию между $N(0,1)$ и $N(0,4)$.

\begin{multline*}
KL(N(0,1) || N(0,4)) = \int_{-\infty}^{+\infty} f_P(t) \cdot (-\ln(f_Q(t)) \dx{t} - \\ -\int_{-\infty}^{+\infty} f_P(t) \cdot (-\ln(f_P(t)) \dx{t} = \int_{-\infty}^{+\infty} f_P(t) \cdot \left( -\ln \frac{f_Q(t)}{f_P(t)} \right) \dx{t}. 
\end{multline*}

Найдём второй множитель

\[ - \ln \frac{f_Q(t)}{f_P(t)}  = -\ln \left(\frac{1}{2} \cdot e^{\tfrac{3}{8} t^2} \right) = \ln2 - \frac{3t^2}{8}.  \]

Теперь добьём дивергенцию 

\[ \int_{-\infty}^{+\infty} f_P(t) (\ln2 - \frac{3t^2}{8}) \dx{t} = \ln2 \cdot 1 - \frac{3}{8} \E(X^2) = \ln2 - \frac{3}{8}. \]

Если заменить натуральный логарифм на двоичный, можно снова уйти в биты.  Но обычно для удобства используется именно натуральный логарифм.

\todo[inline]{Пример про классификацию}
 
 В байесовских методах с помощью $KL$-дивергенции можно оценивать информационный выигрыш при переходе от априорного распределения к апостериорному. В таком контексте формула примет вид 
 
 \[ KL(\b \mid y_{n+1}, y || \b \mid y) =  \int f(\b \mid y_{n+1}, y) \cdot \ln \frac{f(\b \mid y_{n+1}, y)}{f(\b \mid y)}. \]

Полученная в ходе вычислений цифра будет отражать то, какое количество дополнительной информации касательно параметра $\b$ мы получили, пронаблюдав дополнительный $y_{n+1}$. Другой путь осознать насколько полезным оказалось новое наблюдение  --- посмотреть на разность апостериорной и априорной энтропий

\[ \Delta H =  H(\b \mid y_{n+1}, y) - H(\b \mid y).\]

Никто не гарантирует, что эта разность получится положительной.  В случае дивергенции Кульбака-Лейблера мы можем быть уверены, что прирост информации окажется либо нулевым либо положительным.  

Здесь мы можем увидеть разницу между информацией и неопределённостью во всей красе. Энтропия измеряет неопределённость. При поступлении нового наблюдения она может как возрасти так и уменьшиться.  $KL$-дивергенция отражает то, сколько дополнительной информации мы извлекли из нового наблюдения. Прирост информации есть всегда. Даже когда она увеличивает неопределённость. 

Конечно же, если говорить в терминах математических ожиданий, то ожидаемый прирост информации всегда равен ожидаемому уменьшению неопределённости.  Конечно же, при условии, что модель была верно специфицирована.  

По аналогии мы можем применять метрики из теории информации для распределения $y_{new}$, которое мы используем при построении прогнозов.

\todo[inline]{снова монетка и подбрасывания, наблюдение за информацией}

\section{Про поимку шпиона} 

Пришло время срывать маски.  Если на Земле действуют прогрессоры значительно более развитых рас, то в чём будут, скорее всего, состоять их действия? Если правдоподобие хочет, чтобы мы его не заметили, то что оно сделает?

Вариант первый: скажет, что оно это функция потерь. Случаи, когда происходит подобная метаморфоза, мы рассмотрели выше. Сейчас мы попытаемся словить пару более тонких ситуаций.  Чтобы сделать это, немного поколдуем с оценками максимального правдоподобия, которые мы получали максимизируя функцию правдоподобия: 

\[ \hb = \argmax_{\b} L(y \mid \b)  = \argmax_{\b} \prod f(y \mid \b).\]

Обычно мы пользовались логарифмическим правдоподобием. Давайте домножим его на $-1$, и будем искать вместо максимума минимум. Также давайте домножим его на $\frac{1}{n}$. Эта константа ни на что не повлияет, так как мы сможем избавиться от неё после взятия производной, но интерпретации даст много

\[ \hb =\argmax_{\b} \sum \ln f(y_i \mid \b)  =  \argmin_{\b}  \left( -\frac{1}{n} \sum \ln f(y_i \mid \b)  \right).\]

Колдуем дальше. Если бы мы могли заглянуть в хрустальный шар и увидеть там истиное значение параметра $\b_0$, тогда бы по закону больших чисел среднее сходилось бы к математическому ожиданию

\[ -\frac{1}{n} \sum \ln f(y_i \mid \b_0) \to \E(- \ln f(y \mid \b_0)) = H(Y)\]

и задача максимизации правдоподобия была бы эквивалентна задаче минимизации энтропии. К сожалению, у нас нет хрустального шара. Тем не менее, мы обладаем математикой, в которой издревле завалялся классический трюк. Добавим нужное нам слагаемое. Не будем забывать, что чтобы купить что-нибудь нужное, надо продать что-нибудь ненужное. Вычтем из функции то же самое слагаемое. 

\begin{multline*}
\frac{1}{n} \sum -\ln f(y_i \mid \b) = \\ =  \frac{1}{n} \sum (-\ln f(y_i \mid \b) + \ln f(y_i \mid \b_0) - \ln f(y_i \mid \b_0) ) = \\  = \frac{1}{n} \sum \left[log \frac{f(t \mid \b_0)}{f(y \mid \b)} - \ln f(y \mid \b_0) \right].
\end{multline*}

Снова используем закон больших чисел и получим, что 

\begin{multline*}
\frac{1}{n} \sum \left[log \frac{f(t \mid \b_0)}{f(y \mid \b)} - \ln f(y \mid \b_0) \right] \to KL(f(y \mid \b_0 || y \mid \b)) + H(Y).
\end{multline*}

На второе слагаемое мы не можем оказывать никакого влияния своими манипуляциями. Вся наша статистическая работа идёт с $\b$, которое находится в первом слагаемом.  Выходит, что когда мы хотим получить $\b$ максимально близкое к $\b_0$, мы на самом деле минимизируем дивергенцию. 

Итак, в текущей главе мы посмотрели на несколько различных понятий и методов, которые оказались довольно сильно переплетены между собой\footnote{Потяни за нить, за ней потянется клубок.}.  Мы поговорили о том, к чему приводит использование различных функций потерь в контексте прогнозов, а также ввели несколько новых понятий, перекочевавших в теорию вероятностей из теории информации. Оказалось, что эти понятия тесно связаны с такой классической штукой, как правдоподобие. 

В следующей главе мы разовьём идеи, связанные с потерями и поговорим про регуляризацию. Немного позже мы вернёмся к дивергенции и обсудим такую важную штуку, как вариационные приближения. Будет интересно. Но давайте не будем забегать вперёд и для начала прорешаем упражнения к этой главе.  В них надо будет распутать пару клубков. 

\todo[inline]{мем со скуби-ду и типо вот кто ты на самом деле}

\section{Ещё задачи} 

\begin{problem}\label{log_reg}
	Давайте попробуем посмотреть, к каким функциям потерь могут привести нас разные распределения. 
	\begin{enumerate}
		\item Пусть ошибки $\e_i$ в задаче регрессии имеют распределение Лапласа с плотностью распределения 
		
		\[f_{\e}(t) = \frac{1}{2 \sigma} e^{-\frac{|t|}{\sigma}} \] 
		
		Минимизации какой функции потерь в таком случае эквивалентен метод максимального правдоподобия? 
		
		\item Пусть переменная $y_i$ --- это лайки на странице Маши. Она получает их с какой-то интенсивностью $\lambda$, зависящей от числа постов за день $x_i$. То есть, $\lambda = \beta \cdot x_i$. Какую функцию потерь нужно минимизировать, чтобы получить оценку $\beta$, исходя из принципа максимизации правдоподобия? 
		
		\item \todo[inline]{Ещё пунктов! }

	\end{enumerate}
	\begin{sol}
	\begin{enumerate}		
		\item Выписываем правдоподобие
		
		\[ L = \frac{1}{(2 \sigma)^n} \cdot e^{- \sum_{i=1}^n \frac{|y_i - \beta x_i|}{\sigma}} \to \max_{\beta, \sigma} \] 
		
		Прологарифмируем, получим 
		
		\[ \ln L = - n \ln \sigma - \frac{1}{\sigma} \sum_{i=1}^n |y_i - \beta x_i| \to \max_{\beta, \sigma}.\]
		
		Получается, что для максимизации правдоподобия и поиска $\beta$, нам нужно минимизировать абсолютные потери. 
		
		\item  Подобная модель называется пуассоновской регрессией.  Вероятность выпадения конкретного наблюдения составит 
		
		\[ \PP(y = y_i) = \frac{e^{-\beta x_i} (\beta x_i)^{y_i}}{y_i!}.\]
		
		Выписываем функцию правдоподобия 
		
		\[ L = \frac{e^{-\beta \sum x_i} \cdot \beta^{\sum y_i} \cdot x_1^{y_1} \cdot \ldots \cdot x_n^{y_n}}{y_1! \cdot \ldots \cdot y_n!} \to \max_{\beta}. \]
		
		Прологарифмируем
		
		\[\ln L = -\beta \sum x_i + \sum y_i \ln \beta + \sum y_i \ln x_i - \sum y_i! \to \max_{\beta}.\]
		
		Откидываем все константные слагаемые и получаем функцию потерь
		
		\[\beta \sum x_i -  \ln \beta \sum y_i   \to \min_{\beta}. \]
		
		Обратите внимание, что в данном случае можно решить задачу влоб, тогда получится, что $\hat \beta = \frac{\bar x}{\bar y}.$ Выходит, что чувствительность интенсивности лайков к числу постов на стене равна тому, сколько постов приходится на один лайк. Звучит логично. 		
	\end{enumerate}		
	\end{sol}
\end{problem}


\begin{problem}
Исследователь Дмитрий захотел выиграть грант на свои байесовские исследования. По условиям конкурса он должен предложить богатой государственной комиссии какие-нибудь интересные байесовские точечные оценки и объяснить из каких-таких предположений они появились. 

Дмитрий собирается попробовать следующие функции. К чему это приведёт и получит ли бравый исследователь грант?  
\begin{enumerate}
\item $L(\beta, \beta_F) = (\beta - \hat \beta)^3$
\item $L(\beta, \beta_F) = (\beta - \hat \beta)^2 + \lambda \cdot  \hat \b^2 $
\item $L(\beta, \beta_F) = (\phi(\beta) - \phi(\hb)) - \phi'(\hb) \cdot(\b - \hb) $, где $\phi$ --- любая бесконечно дифференцируемая функция. 
\item $L(\b, \hb) = \begin{cases} \alpha \cdot (\hb - \b), \text{ если } \hb > \b \\ (1 - \alpha) \cdot (\b - \hb), \text{ если } \hb \le \b . \end{cases}$
\item $L(\b , \hb) = \frac{|\b - \hb|}{\b} $. Как можно найти оптимум для такой функции потерь? 
\end{enumerate}
\begin{sol}

\begin{enumerate}
\item 

Пусть $L(\beta, \hb) = (\hb  - \beta)^3$, тогда 

\[ \int (\beta - t)^3 \cdot f(\beta \mid y) \dx{\beta} \to \min_{t} \] 

Найдём производную по $t$

\begin{align*}
\frac{\partial }{\partial t} \left(  \int (\beta - t)^3 \cdot f(\beta \mid y) \dx{\beta}   \right) &= - 3 \cdot \int (\beta - t)^2 \cdot f(\beta \mid y) \dx{\beta} = 0 \\ 
\int (\beta^2 - 2\b t + t^2) f(\beta \mid y) \dx{\b} &= t^2 - 2t \E(\b \mid y) + \E(\b^2 \mid y) = 0\\
D = 4 \E^2(\b \mid y) - 4 \E(\b^2 \mid y) &= - 4 \cdot\Var(\beta \mid y) < 0 
\end{align*}

Действительного оптимального прогноза не существует. За такие исследования Дмитрий грант явно не получит...

\item Пусть $L(\beta, \hb) = (\beta - \hb)^2 + \lambda \b_F^2$, тогда 

\[ \int ((\beta - t)^2 + \lambda t^2) \cdot f(\beta \mid y) \dx{\beta} \to \min_{t} \] 

Найдём производную по $t$

\begin{multline*}
\frac{\partial }{\partial t} \left(  \int ((\beta - t)^2 + \lambda t^2) \cdot f(\beta \mid y) \dx{\beta}   \right) = \\ =  2 \cdot \int (-2(\beta - t) + 2 \lambda t) \cdot f(\beta \mid y) \dx{\beta} = 0 \\ 
\int \beta \cdot f(\beta \mid y) \dx{\beta} - \int (\lambda + 1) \cdot t \cdot f(\beta \mid y) \dx{\beta} = \E(\beta \mid y) - (\lambda + 1) \cdot t \cdot 1 =  0 \\ \quad \Rightarrow \quad t = \frac{\E(\beta \mid y)}{1 + \lambda}  
\end{multline*}

Хммм... Получилось что-то дельное. Чем большее значение принимает $\lambda$, тем сильнее наша точечная байесовская оценка стягивается к нулю. Уже лучше, но на грант всё ещё не тянет.  Запомните этот пример. В следущей главе мы будем обсуждать регуляризацию, и эта формула снова всплывёт. 

\item Пусть $L(\beta, \hb) = (\phi(\beta) - \phi(\hb)) - \phi'(\hb) \cdot(\b - \hb) $, где $\phi$ --- любая бесконечно дифференцируемая функция, тогда 

\[ \int ((\phi(\beta) - \phi(t)) - \phi'(t) \cdot(\b - t) )\cdot f(\beta \mid y) \dx{\beta} \to \min_{t} \] 

Найдём производную по $t$

\begin{multline*}
\frac{\partial }{\partial t} \left(  \int (\phi(\beta) - \phi(t) - \phi'(t) \cdot(\b - t)) \cdot  f(\beta \mid y) \dx{\beta}   \right) = \\ =  \int (-\phi'(t) - \phi''(t)\cdot(\b - t) + \phi'(t)) \cdot f(\b \mid y) \dx{\b} = 0  \\ 
\phi''(t)\cdot(\E(\b \mid t) - t) = 0  \quad \Rightarrow \quad t = \E(\beta \mid y)  
\end{multline*} 

Получаем интересный вывод. Математическое ожидание доставляет минимум более широкому классу функций потерь, а не только квадратичной функции. Приведённая в этой задаче функция называется дивиргенцией (функцией потерь) Брегмана.

В случае $\phi(x) = x^2$, получаем уже знакомую нам квадратичную функцию потерь: 

\[ \phi(\b) - \phi(t) - \phi'(t)\cdot(\b-t) = \b^2 - t^2 - 2t \cdot (\b-t) =  (\b-t)^2    \] 

В случае $\phi(x) = p \cdot \ln p$ можно получить дивергенцию Кульбака-Лейбнера, а в случае $\phi(x) = - \ln(p)$ дивергенцию Итакура-Саито. Но это уже совсем другая история.  Если до Дмитрия это никто не заметил, наклёвывается грант. 

\item Мы имеем дело с квантильными потерями потерь. Они почти как MAE, но только положительная и отрицательная ошибки, имеют разные цены.  Например, в такой ситуации недопрогноз более критичен: 

\begin{center}
\begin{tikzpicture}
% оси
\draw [->] (-2.2,0) -- (2.2,0);
\draw [->] (0,-0.2) -- (0,3.5);
% график
\draw [blue, thick, domain=0:2] plot (\x, 0.5*\x);
\draw [blue, thick, domain=-1.5:0] plot (\x, -2*\x);
% подписи
\node [below right] at (2.2,0) {\small $error$};
\node [right] at (0,3.5) {\small $L(error)$};
\end{tikzpicture}
\end{center} 

Давайте выясним, что нам даёт в качестве прогноза, квантильная функция потерь. 

\[ \int_{\b > t} \alpha \cdot (\b - t) f(\b \mid y) \dx{\beta} + \int_{\b \le t} (1 - \alpha) \cdot (t - \b) f(\b \mid y) \dx{\b} \to \min_t \]

Берём производную, приравниваем к нулю

\[ -\int_{\b > t} \alpha f(\b \mid y) \dx{\beta}  + \int_{\b \le t} (1- \a) f(\b \mid y) \dx{\beta}. \] 

Получается, что 

\[ \alpha \PP(\b > t \mid y) = (1 - \alpha) \PP(\b \le y \mid y) \Rightarrow \PP(\b < t \mid y) = \alpha.\]

На выходе получаем, что оптимальный прогноз находится как условный квантиль уровня $\a$ для нашего апостериорного распределения. 


\item 

\todo[inline]{С этим пунктом есть проблемы.}

Эта функция потерь называетя MAPE. Она довольно часто применяется, если нам принципиально, на сколько процентов мы ошиблись, а не абсолютное значение. Если мы предсказали $1$, а в реальности получилось $10$ --- это не то же самое, что мы предсказали $1001$, а получилось $1010$. В первом случае ошибка в процентном плане катастрофичнее, и MAPE улавливает это. Давайте проанализируем эту функцию потерь. 

Если по аналогии с MAE попробовать изучить эту функцию потерь, мы получим, что прогноз возникает из странного равенства

\[ 
\int_{\b \le t} \frac{1}{\b} f(\b \mid y) \dx{\b} = \int_{\b > t} \frac{1}{\b} f(\b \mid y} \dx{\b})
\]

\todo[inline]{И что? Можно ил это интерпретировать как exp(Med(ln b)) и придумать грамотное процентное описание? ХЕЛП!}

Возникает новый вопрос. MAPE не дифференцируемая, более того при нулевом правильном ответе мы уходим в бесконечность. Как правильно оптимизировать такую функцию? 

На практике можно применить следущую хитрость. Давайте попробуем преобразовать наше $\b$ функцией $f(\b)$ так, чтобы свести задачу к оптимизации абсолютной функции потерь. 

Мы хотим, чтобы $\frac{|\b -\hat \b|}{\b} \approx |f(\b) - f(\hb)|$.

Если разложить $f$ в ряд Тэйлора до первого члена, то можно получить, что 

\[\frac{|\b -\hat \b|}{\b} \approx |f(\b) - f(\hb)| \approx |f'(\b)| |\b - \hb|.\]

Чтобы получить после взятия производной MAPE, нам нужно, чтобы $f'(\b) = \frac{1}{\b}$. То есть, если мы хотим оптимизировать MAPE, нам нужно прологарифмировать таргеты, обучиться на логарифмах, оптимизируя MAE, а после восстановить предсказания с помощью экспонент. 
\end{enumerate}
\end{sol}
\end{problem}

\begin{problem}\label{log_reg_2}
Перед Винни-Пухом стоит задача классифицировать пчёл на правильных и неправильных. В его распоряжении есть выборка $(y_i,x_i)$. Переменная $y_i$ принимает значение $1$, если пчела правильная и значение $0$, если пчела неправильная. Переменная $x_i$ --- это густота мёда пчелы. 

Выборка собрана, исследовательский энтузиазм зашкаливает. Есть только одна беда. Непонятно какую именно функцию потерь лучше использовать. Однако есть варианты: 

\begin{enumerate}\label{abs_0_1}
\item $L(y,a(x)) = (y - a(x))^2$
\item $L(y,a(x)) = |y - a(x)|$

\end{enumerate}

Винни очень бы хотелось на выходе обязательно получить оценку вероятности принадлежности пчелы к определённому классу. Какую из функций лучше использовать исследователю? 

\begin{sol}

\begin{enumerate}
\item Пусть $L(y,a(x)) = (y - a(x))^2$. Тогда получаем задачу 

\[
\sum_{y \in Y} (y - t)^2 \PP(y \mid x) \to \min_t
\]

Снова обозначим $ p = \PP(y = 1 \mid x)$, а после возьмём производную

\begin{align*}
p \cdot (1 - t)^2 + (1 - p) (-t^2) &\to \min_t \\
-2p(1-t) - 2(1-p)\cdot(-t) = 0 \quad &\Rightarrow \quad t = p.
\end{align*}

Получается, что функция потерь удовлетворяет условиям Пуха. 

\item Пусть $L(y,a(x)) = |y - a(x)|$. Тогда получаем задачу 


\begin{align*}
\sum_{y \in Y} |y - t| \PP(y \mid x) &\to \min_t \\ 
p \cdot |1-t| + (1-p) \cdot |-t| & \to \min_t
\end{align*}

Рассмотрим ситуации, в которых $t<0$, $0 < t < 1$, $t >1$. 

\todo[inline]{Дорешать}

В конечном итоге видим, что величина $t$ принимает значения либо $1$ либо $0$ и никак не тянет на оценку вероятности. Такая функция не удовлетворяет условию Пуха. 


\end{enumerate}
\end{sol}
\end{problem}

\todo[inline]{Доделать про Ивана и Царя!}

\begin{problem}
	Царь предлагает Ивану не просто голову с плеч или половину царства, но ещё и промежуточный вариант, ничего (0). Функция как бы склеивается из кусочков. Сюда же задача про царя-извращенца, который платит за то, что мы не угадали
	\begin{sol} 
		
		
	\end{sol} 
\end{problem}


\begin{problem}
Случайная величина $Y$ принимает два значения: $0$ с вероятностью $p$ и $1$ с вероятностью $1-p$. Постройте график зависимости энтропии от $p$. При каком $p$ энтропия будет максимальной? Проинтерпретируйте это. Является ли функция монотонной? Выпуклой? 	
\begin{sol} 
	
\end{sol} 
\end{problem}


\begin{problem}
	ещё упражнение про энтропию - ?  
	добавить везде пункт с поиском перплексии 
	\begin{sol} 
		
	\end{sol} 
\end{problem}

 % энтропия для непрерывной случайной величины - ? 
  
 \begin{problem}
Найдите дивергенцию Кульбака-Лейбнера, если она определена, 

\begin{enumerate}
\item  из биномиального $Bin(\frac{1}{3}, 2)$ в равновероятное на $0,1,2$;
\item из равновероятного на $0,1,2$ в биномиального $Bin(\frac{1}{3}, 2)$;
\item из $N(0,l)$ в $N(0, \sigma^2)$;
\item из  $N(0, \sigma^2)$ в $N(0,l)$;
\item из $N(0,1)$ в $Exp(1)$;
\item из $Exp(1)$ в $N(0,l)$. 
\end{enumerate}

	\begin{sol} 
		
	\end{sol} 
\end{problem}



\begin{problem}
	
	
	\begin{sol} 
		
		
	\end{sol} 
\end{problem}

\todo[inline]{Добавить упражнений на взаимосвязь всего этого в один клубок и добавить байесовщинки :3 }

\todo[inline]{Поискать какие-нибудь задачи дожития с экспоненциальным распределением}


% \Closesolutionfile{solution_file}
% \input{solutions1}

\end{document}

